import kagglehub

# Download latest version
path = kagglehub.dataset_download("clmentbisaillon/fake-and-real-news-dataset")

print("Path to dataset files:", path)
import os
import shutil # Added for copying files

def download_dataset(target_dir='./data/raw/'):
    # The dataset has already been downloaded and extracted by kagglehub
    # in the previous cell. The path to the extracted dataset is available
    # in the 'path' variable from the global scope of the notebook.

    # Ensure the target directory exists and is empty before copying
    if os.path.exists(target_dir):
        shutil.rmtree(target_dir)
    os.makedirs(target_dir)

    # Copy files from the kagglehub download location to the target directory
    source_path = path # 'path' is a global variable from the previous cell

    print(f"Copying dataset files from '{source_path}' to '{target_dir}'...")

    for item in os.listdir(source_path):
        s = os.path.join(source_path, item)
        d = os.path.join(target_dir, item)
        if os.path.isdir(s):
            # shutil.copytree requires Python 3.8+ for dirs_exist_ok.
            # Using a safer approach for older Python versions or explicit overwrite.
            # Given Colab's Python version, dirs_exist_ok=True should be fine.
            shutil.copytree(s, d, dirs_exist_ok=True)
        else:
            shutil.copy2(s, d) # Copy files, preserving metadata

    print("Dataset files copied successfully.")
download_dataset()

# %% [markdown]
# # Projet Détection de Fake News
# ## Analyse et Modélisation

# %% [markdown]
# ### 1. Importation des Librairies
# -%%
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# NLP libraries
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import re
import string

# Machine Learning
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import (classification_report, confusion_matrix,
                           accuracy_score, f1_score, roc_auc_score, roc_curve)
from sklearn.pipeline import Pipeline

# Deep Learning
import torch
from transformers import (AutoTokenizer, AutoModelForSequenceClassification,
                         Trainer, TrainingArguments, BertTokenizer, BertModel)
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn

# Visualisation
from wordcloud import WordCloud
import plotly.express as px
import plotly.graph_objects as go

# Télécharger les ressources NLTK
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-english')
nltk.download('punkt_tab') # Added to resolve LookupError

# -%%
# Configuration
plt.style.use('ggplot')
sns.set_palette("husl")
pd.set_option('display.max_columns', None)
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
torch.manual_seed(RANDOM_SEED)

# -%%
# Chargement des données
true_news = pd.read_csv('data/raw/True.csv')
fake_news = pd.read_csv('data/raw/Fake.csv')

# Ajout des labels
true_news['label'] = 0  # 0 = vraies nouvelles
fake_news['label'] = 1  # 1 = fausses nouvelles

# Concaténation
df = pd.concat([true_news, fake_news], ignore_index=True)

# Mélanger les données
df = df.sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)

# -%%
# Aperçu des données
print("Dimensions du dataset :", df.shape)
print("\n5 premières lignes :")
display(df.head())

print("\nInformations sur les colonnes :")
print(df.info())

print("\nStatistiques descriptives :")
print(df.describe())

# -%%
# Vérification des valeurs manquantes
print("Valeurs manquantes par colonne :")
print(df.isnull().sum())

# -%%
# Distribution des classes
plt.figure(figsize=(10, 6))
ax = sns.countplot(x='label', data=df)
plt.title('Distribution des Classes (0=Vrai, 1=Faux)', fontsize=16)
plt.xlabel('Classe')
plt.ylabel('Nombre d\'articles')

# Ajouter les pourcentages
total = len(df)
for p in ax.patches:
    percentage = f'{100 * p.get_height()/total:.1f}%'
    x = p.get_x() + p.get_width() / 2
    y = p.get_height() + 0.02 * total
    ax.annotate(percentage, (x, y), ha='center', fontsize=12)

plt.show()

# -%%
# Analyse par sujet
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# Sujets des vraies nouvelles
true_subjects = true_news['subject'].value_counts()
axes[0].pie(true_subjects.values, labels=true_subjects.index, autopct='%1.1f%%')
axes[0].set_title('Distribution des Sujets - Vraies Nouvelles')

# Sujets des fausses nouvelles
fake_subjects = fake_news['subject'].value_counts()
axes[1].pie(fake_subjects.values, labels=fake_subjects.index, autopct='%1.1f%%')
axes[1].set_title('Distribution des Sujets - Fausses Nouvelles')

plt.tight_layout()
plt.show()

# %% [markdown]
# ### 3. Analyse Textuelle Exploratoire
# -%%
# Fonctions d\'analyse textuelle
def analyze_text_stats(df):
    """Calcule des statistiques textuelles"""
    # stats = pd.DataFrame() # This line is not needed

    # Longueur des textes
    df['text_length'] = df['text'].apply(len)
    df['word_count'] = df['text'].apply(lambda x: len(x.split()))
    df['sentence_count'] = df['text'].apply(lambda x: len(x.split('.')))
    # Fix for NaN: ensure x.split() is not empty before calculating mean
    df['avg_word_length'] = df['text'].apply(lambda x: np.mean([len(w) for w in x.split()]) if len(x.split()) > 0 else 0)

    # Ponctuation
    df['exclamation_count'] = df['text'].apply(lambda x: x.count('!'))
    df['question_count'] = df['text'].apply(lambda x: x.count('?'))
    df['uppercase_ratio'] = df['text'].apply(
        lambda x: sum(1 for c in x if c.isupper()) / len(x) if len(x) > 0 else 0
    )

    return df

# -%%
# Application de l\'analyse
df = analyze_text_stats(df)

# -%%
# Visualisation des statistiques par classe
fig, axes = plt.subplots(2, 3, figsize=(18, 10))
features = ['text_length', 'word_count', 'sentence_count',
            'avg_word_length', 'exclamation_count', 'uppercase_ratio']
titles = ['Longueur du texte', 'Nombre de mots', 'Nombre de phrases',
          'Longueur moyenne des mots', "Nombre d'exclamations", "Ratio de majuscules"]

for idx, (feature, title) in enumerate(zip(features, titles)):
    ax = axes[idx//3, idx%3]
    sns.boxplot(x='label', y=feature, data=df, ax=ax)
    ax.set_title(f'{title} par Classe')
    ax.set_xlabel('Classe (0=Vrai, 1=Faux)')

plt.tight_layout()
plt.show()

# -%%
# Nuages de mots
def generate_wordcloud(text, title):
    """Génère un nuage de mots"""
    wordcloud = WordCloud(width=800, height=400,
                         background_color='white',
                         max_words=100,
                         colormap='viridis').generate(text)

    plt.figure(figsize=(10, 6))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title(title, fontsize=16)
    plt.axis('off')
    plt.show()

# -%%
# Nuage pour les vraies nouvelles
true_text = ' '.join(df[df['label'] == 0]['text'].astype(str))
generate_wordcloud(true_text, 'Nuage de Mots - Vraies Nouvelles')

# Nuage pour les fausses nouvelles
fake_text = ' '.join(df[df['label'] == 1]['text'].astype(str))
generate_wordcloud(fake_text, 'Nuage de Mots - Fausses Nouvelles')

# %% [markdown]
# ### 4. Prétraitement du Texte
# -%%
class TextPreprocessor:
    """Classe pour le prétraitement du texte"""

    def __init__(self):
        self.stop_words = set(stopwords.words('english'))
        self.lemmatizer = WordNetLemmatizer()

    def clean_text(self, text):
        """Nettoie et prétraite le texte"""
        if not isinstance(text, str):
            return ""

        # Convertir en minuscules
        text = text.lower()

        # Supprimer les URLs
        text = re.sub(r'https?://\S+|www\.\S+', '', text)

        # Supprimer les mentions et hashtags
        text = re.sub(r'@\w+|#\w+', '', text)

        # Supprimer la ponctuation
        text = re.sub(f'[{re.escape(string.punctuation)}]', ' ', text)

        # Supprimer les chiffres
        text = re.sub(r'\d+', '', text)

        # Supprimer les espaces multiples
        text = re.sub(r'\s+', ' ', text).strip()

        return text

    def tokenize_and_lemmatize(self, text):
        """Tokenisation et lemmatisation"""
        tokens = word_tokenize(text)

        # Supprimer les stopwords et lemmatiser
        tokens = [self.lemmatizer.lemmatize(word)
                 for word in tokens
                 if word not in self.stop_words and len(word) > 2]

        return ' '.join(tokens)

    def preprocess_pipeline(self, text):
        """Pipeline complet de prétraitement"""
        cleaned = self.clean_text(text)
        processed = self.tokenize_and_lemmatize(cleaned)
        return processed

# -%%
# Application du prétraitement
print("Prétraitement du texte en cours...")
preprocessor = TextPreprocessor()

# Appliquer sur un échantillon pour tester
sample_text = df.iloc[0]['text']
print("Texte original (extrait):")
print(sample_text[:500])
print("\nTexte prétraité:")
print(preprocessor.preprocess_pipeline(sample_text)[:500])

# -%%
# Application sur tout le dataset (peut prendre du temps)
df['cleaned_text'] = df['text'].apply(preprocessor.preprocess_pipeline)

# Ensure the directory exists before saving
import os
os.makedirs('data/processed', exist_ok=True)

# Sauvegarder les données nettoyées
df.to_csv('data/processed/cleaned_news.csv', index=False)
print("Données nettoyées sauvegardées!")

# %% [markdown]
# ### 5. Feature Engineering
# -%%
class FeatureEngineer:
    """Classe pour l'ingénierie des features"""

    def __init__(self):
        pass

    def extract_linguistic_features(self, text):
        """Extrait des features linguistiques"""
        features = {}

        # Features basiques
        words = text.split()
        sentences = text.split('.')

        features['char_count'] = len(text)
        features['word_count'] = len(words)
        features['sentence_count'] = len([s for s in sentences if s.strip()])
        features['avg_word_length'] = np.mean([len(w) for w in words]) if words else 0
        features['avg_sentence_length'] = features['word_count'] / features['sentence_count'] if features['sentence_count'] > 0 else 0

        # Complexité lexicale
        unique_words = set(words)
        features['lexical_diversity'] = len(unique_words) / len(words) if words else 0

        # Ponctuation émotionnelle
        features['exclamation_ratio'] = text.count('!') / features['word_count'] if features['word_count'] > 0 else 0
        features['question_ratio'] = text.count('?') / features['word_count'] if features['word_count'] > 0 else 0

        # Majuscules
        features['uppercase_ratio'] = sum(1 for c in text if c.isupper()) / len(text) if len(text) > 0 else 0

        # Mots subjectifs (liste basique)
        subjective_words = ['think', 'believe', 'feel', 'seem', 'appear', 'probably',
                           'maybe', 'perhaps', 'actually', 'really', 'very', 'extremely']
        features['subjective_word_ratio'] = sum(1 for w in words if w.lower() in subjective_words) / len(words) if words else 0

        return features

    def extract_all_features(self, texts):
        """Extrait toutes les features pour une liste de textes"""
        all_features = []
        for text in texts:
            features = self.extract_linguistic_features(text)
            all_features.append(features)

        return pd.DataFrame(all_features)

# -%%
# Extraction des features
print("Extraction des features linguistiques...")
feature_engineer = FeatureEngineer()
linguistic_features = feature_engineer.extract_all_features(df['cleaned_text'].tolist())

# Concaténer avec le dataframe original
df_features = pd.concat([df.reset_index(drop=True), linguistic_features.reset_index(drop=True)], axis=1)

# -%%
# Analyse de corrélation des features
numeric_features = ['char_count', 'word_count', 'sentence_count', 'avg_word_length',
                    'lexical_diversity', 'exclamation_ratio', 'question_ratio',
                    'uppercase_ratio', 'subjective_word_ratio', 'label']

corr_matrix = df_features[numeric_features].corr()

plt.figure(figsize=(12, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f')
plt.title('Matrice de Corrélation des Features', fontsize=16)
plt.tight_layout()
plt.show()

# %% [markdown]
# ### 6. Préparation des Données pour ML
# -%%
# Séparation features/target
X_text = df_features['cleaned_text']
X_features = df_features.drop(['title', 'text', 'subject', 'date', 'cleaned_text', 'label'], axis=1, errors='ignore')
y = df_features['label']

# Séparation train/test
X_train_text, X_test_text, X_train_feat, X_test_feat, y_train, y_test = train_test_split(
    X_text, X_features, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y
)

print(f"Train size: {len(X_train_text)}")
print(f"Test size: {len(X_test_text)}")

# %% [markdown]
# ### 7. Modélisation - Approche Classique
# -%%
# Vectorisation TF-IDF
print("Vectorisation TF-IDF...")
tfidf_vectorizer = TfidfVectorizer(
    max_features=5000,
    ngram_range=(1, 2),  # Unigrammes et bigrammes
    min_df=5,
    max_df=0.7
)

X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_text)
X_test_tfidf = tfidf_vectorizer.transform(X_test_text)

# Combiner avec les features linguistiques
from scipy.sparse import hstack

X_train_combined = hstack([X_train_tfidf, X_train_feat.values.astype('float32')])
X_test_combined = hstack([X_test_tfidf, X_test_feat.values.astype('float32')])

# -%%
# Définition des modèles à tester
models = {
    'Logistic Regression': LogisticRegression(random_state=RANDOM_SEED, max_iter=1000),
    'Naive Bayes': MultinomialNB(),
    'Random Forest': RandomForestClassifier(random_state=RANDOM_SEED, n_jobs=-1),
    'SVM': SVC(random_state=RANDOM_SEED, probability=True)
}

# -%%
# Évaluation des modèles
results = []

for name, model in models.items():
    print(f"\nEntraînement de {name}...")

    # Entraînement
    model.fit(X_train_combined, y_train)

    # Prédictions
    y_pred = model.predict(X_test_combined)
    y_pred_proba = model.predict_proba(X_test_combined)[:, 1] if hasattr(model, 'predict_proba') else None

    # Métriques
    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else None

    # Rapport de classification
    print(f"\n{name} - Rapport de Classification:")
    print(classification_report(y_test, y_pred))

    # Matrice de confusion
    cm = confusion_matrix(y_test, y_pred)

    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'Matrice de Confusion - {name}')
    plt.ylabel('Vrai')
    plt.xlabel('Prédit')
    plt.show()

    # Sauvegarde des résultats
    results.append({
        'Model': name,
        'Accuracy': accuracy,
        'F1-Score': f1,
        'ROC-AUC': roc_auc
    })

# -%%
# Comparaison des modèles
results_df = pd.DataFrame(results)
print("\nComparaison des modèles:")
display(results_df.sort_values('F1-Score', ascending=False))

# Visualisation
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

metrics = ['Accuracy', 'F1-Score', 'ROC-AUC']
for idx, metric in enumerate(metrics):
    ax = axes[idx]
    results_df.sort_values(metric, ascending=True).plot(
        x='Model', y=metric, kind='barh', ax=ax, legend=False
    )
    ax.set_title(f'Comparaison des Modèles - {metric}')
    ax.set_xlabel(metric)

plt.tight_layout()
plt.show()

# %% [markdown]
# ### 8. Optimisation avec GridSearchCV
# -%%
# Optimisation du meilleur modèle
print("\nOptimisation du modèle Random Forest...")

param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [None, 20, 30],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2]
}

rf = RandomForestClassifier(random_state=RANDOM_SEED, n_jobs=-1)
grid_search = GridSearchCV(
    rf, param_grid, cv=5, scoring='f1', n_jobs=-1, verbose=1
)

grid_search.fit(X_train_combined, y_train)

print(f"\nMeilleurs paramètres: {grid_search.best_params_}")
print(f"Meilleur score F1: {grid_search.best_score_:.4f}")

# -%%
# Évaluation du modèle optimisé
best_rf = grid_search.best_estimator_
y_pred_best = best_rf.predict(X_test_combined)

print("Rapport de Classification - Random Forest Optimisé:")
print(classification_report(y_test, y_pred_best))

# Feature importance
if hasattr(best_rf, 'feature_importances_'):
    # Pour les features TF-IDF
    feature_names = list(tfidf_vectorizer.get_feature_names_out()) + list(X_train_feat.columns)

    # Prendre les top 20 features
    importances = best_rf.feature_importances_
    top_indices = np.argsort(importances)[-20:]

    plt.figure(figsize=(10, 8))
    plt.barh(range(len(top_indices)), importances[top_indices])
    plt.yticks(range(len(top_indices)), [feature_names[i] for i in top_indices])
    plt.xlabel('Importance')
    plt.title('Top 20 Features - Random Forest')
    plt.tight_layout()
    plt.show()

# %% [markdown]
# ### 9. Approche Deep Learning avec BERT
# -%%
# Vérification de la disponibilité du GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Device utilisé: {device}")

# -%%
# Préparation des données pour BERT
class NewsDataset(Dataset):
    """Dataset pour BERT"""

    def __init__(self, texts, labels, tokenizer, max_length=128):
        self.texts = texts.reset_index(drop=True) if isinstance(texts, pd.Series) else texts
        self.labels = labels.reset_index(drop=True) if isinstance(labels, pd.Series) else labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]

        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

# -%%
# Initialisation du tokenizer BERT
print("Chargement du tokenizer BERT...")
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

# Création des datasets
train_dataset = NewsDataset(X_train_text, y_train, tokenizer)
test_dataset = NewsDataset(X_test_text, y_test, tokenizer)

# Création des dataloaders
batch_size = 16
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size)

# -%%
# Définition du modèle BERT
class BERTClassifier(nn.Module):
    """Classificateur basé sur BERT"""

    def __init__(self, n_classes=2):
        super(BERTClassifier, self).__init__()
        self.bert = BertModel.from_pretrained('bert-base-uncased')
        self.drop = nn.Dropout(p=0.3)
        self.fc = nn.Linear(self.bert.config.hidden_size, n_classes)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output
        output = self.drop(pooled_output)
        return self.fc(output)

# -%%
# Entraînement du modèle BERT
def train_bert_model(model, train_loader, val_loader, epochs=3):
    """Fonction d'entraînement pour BERT"""

    model = model.to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)
    criterion = nn.CrossEntropyLoss().to(device)

    train_losses = []
    val_accuracies = []

    for epoch in range(epochs):
        print(f"\nEpoch {epoch+1}/{epochs}")

        # Phase d'entraînement
        model.train()
        total_loss = 0

        for batch in train_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            optimizer.zero_grad()
            outputs = model(input_ids, attention_mask)
            loss = criterion(outputs, labels)

            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        avg_train_loss = total_loss / len(train_loader)
        train_losses.append(avg_train_loss)

        # Phase de validation
        model.eval()
        correct = 0
        total = 0

        with torch.no_grad():
            for batch in val_loader:
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                labels = batch['labels'].to(device)

                outputs = model(input_ids, attention_mask)
                _, predicted = torch.max(outputs.data, 1)

                total += labels.size(0)
                correct += (predicted == labels).sum().item()

        val_accuracy = correct / total
        val_accuracies.append(val_accuracy)

        print(f"Train Loss: {avg_train_loss:.4f}, Val Accuracy: {val_accuracy:.4f}")

    return model, train_losses, val_accuracies

# -%%
# Entraînement (commenté car long à exécuter)
# Décommente pour exécuter

"""
print("Entraînement du modèle BERT...")
bert_model = BERTClassifier()
bert_model, train_losses, val_accuracies = train_bert_model(
    bert_model, train_loader, test_loader, epochs=3
)

# Visualisation de la courbe d'apprentissage
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(train_losses, label='Train Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Courbe de Loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(val_accuracies, label='Validation Accuracy', color='orange')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Courbe d\'Accuracy')
plt.legend()

plt.tight_layout()
plt.show()
"""

# %% [markdown]
# ### 10. Évaluation et Interprétation
# -%%
# Fonction pour évaluer un modèle
def evaluate_model(model, X_test, y_test, model_type='sklearn'):
    """Évaluation complète d'un modèle"""

    if model_type == 'sklearn':
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test)[:, 1]

        # Métriques
        print("Rapport de Classification:")
        print(classification_report(y_test, y_pred))

        # Matrice de confusion
        cm = confusion_matrix(y_test, y_pred)

        fig, axes = plt.subplots(1, 2, figsize=(12, 5))

        # Matrice de confusion
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0])
        axes[0].set_title('Matrice de Confusion')
        axes[0].set_ylabel('Vrai')
        axes[0].set_xlabel('Prédit')

        # Courbe ROC
        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
        roc_auc = roc_auc_score(y_test, y_pred_proba)

        axes[1].plot(fpr, tpr, color='darkorange', lw=2,
                    label=f'ROC curve (AUC = {roc_auc:.2f})')
        axes[1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
        axes[1].set_xlim([0.0, 1.0])
        axes[1].set_ylim([0.0, 1.05])
        axes[1].set_xlabel('False Positive Rate')
        axes[1].set_ylabel('True Positive Rate')
        axes[1].set_title('Courbe ROC')
        axes[1].legend(loc="lower right")

        plt.tight_layout()
        plt.show()

        return {
            'predictions': y_pred,
            'probabilities': y_pred_proba,
            'confusion_matrix': cm,
            'roc_auc': roc_auc
        }

# -%%
# Évaluation du modèle optimisé
print("Évaluation du modèle Random Forest optimisé...")
rf_results = evaluate_model(best_rf, X_test_combined, y_test)

# %% [markdown]
# ### 11. Explicabilité avec SHAP
# -%%
# Installation et import de SHAP
try:
    import shap
    print("SHAP importé avec succès!")

    # Calcul des valeurs SHAP (sur un échantillon pour la performance)
    sample_idx = np.random.choice(X_test_combined.shape[0], 100, replace=False)
    X_sample = X_test_combined[sample_idx].toarray() if hasattr(X_test_combined, 'toarray') else X_test_combined[sample_idx]

    # Création de l\'explainer
    explainer = shap.TreeExplainer(best_rf)
    shap_values = explainer.shap_values(X_sample)

    # Visualisation
    plt.figure(figsize=(12, 8))
    shap.summary_plot(shap_values[1], X_sample,
                     feature_names=feature_names[:X_sample.shape[1]],
                     show=False)
    plt.title('Importance des Features - SHAP', fontsize=16)
    plt.tight_layout()
    plt.show()

except ImportError:
    print("SHAP n'est pas installé. Pour l'installer: pip install shap")

# Alternative avec LIME
try:
    from lime.lime_text import LimeTextExplainer
    from lime import lime_text
    from sklearn.pipeline import make_pipeline

    print("\nUtilisation de LIME pour l'explicabilité...")

    # Création d'un pipeline avec le vectorizer et le modèle
    pipeline = make_pipeline(tfidf_vectorizer, best_rf)

    # Création de l\'explainer LIME
    explainer = LimeTextExplainer(class_names=['Vrai', 'Faux'])

    # Exemple d'explication sur un texte
    idx = 0
    exp = explainer.explain_instance(
        X_test_text.iloc[idx],
        pipeline.predict_proba,
        num_features=10
    )

    # Affichage de l\'explication
    print(f"\nExplication LIME pour l'exemple {idx}:")
    print(f"Texte (extrait): {X_test_text.iloc[idx][:200]}...")
    print(f"Prédiction: {'Faux' if pipeline.predict([X_test_text.iloc[idx]])[0] == 1 else 'Vrai'}")
    print("\nFeatures les plus importantes:")
    exp.as_list()

except ImportError:
    print("LIME n'est pas installé. Pour l'installer: pip install lime")

# %% [markdown]
# ### 12. Création d'un Pipeline de Production
# -%%
from sklearn.base import BaseEstimator, TransformerMixin
import pickle
import joblib

# -%%
# Création d'un pipeline complet
class TextPreprocessorTransformer(BaseEstimator, TransformerMixin):
    """Transformer pour le prétraitement du texte"""

    def __init__(self):
        self.preprocessor = TextPreprocessor()

    def fit(self, X, y=None):
        return self

    def transform(self, X, y=None):
        return [self.preprocessor.preprocess_pipeline(text) for text in X]

# -%%
# Pipeline final
final_pipeline = Pipeline([
    ('preprocessor', TextPreprocessorTransformer()),
    ('vectorizer', TfidfVectorizer(
        max_features=5000,
        ngram_range=(1, 2),
        min_df=5,
        max_df=0.7
    )),
    ('classifier', RandomForestClassifier(
        n_estimators=200,
        max_depth=30,
        min_samples_split=2,
        min_samples_leaf=1,
        random_state=RANDOM_SEED,
        n_jobs=-1
    ))
])

# -%%
# Entraînement du pipeline final
print("Entraînement du pipeline final...")
final_pipeline.fit(X_train_text, y_train)

# Évaluation
y_pred_final = final_pipeline.predict(X_test_text)
print("\nPerformance du pipeline final:")
print(classification_report(y_test, y_pred_final))

# -%%
# Sauvegarde du modèle
print("\nSauvegarde du modèle...")
joblib.dump(final_pipeline, 'models/fake_news_pipeline.pkl')
joblib.dump(tfidf_vectorizer, 'models/tfidf_vectorizer.pkl')
joblib.dump(best_rf, 'models/random_forest_model.pkl')

# Sauvegarde des données importantes
with open('models/feature_names.pkl', 'wb') as f:
    pickle.dump(feature_names, f)

print("Modèles sauvegardés dans le dossier 'models/'")

# %% [markdown]
# ### 13. Création d'une Interface Utilisateur Simple
# -%%
# Code pour une interface Streamlit (fichier séparé)
streamlit_code = '''
# app.py
import streamlit as st
import joblib
import pandas as pd
import numpy as np

# Chargement du modèle
@st.cache_resource
def load_model():
    pipeline = joblib.load("models/fake_news_pipeline.pkl")
    return pipeline

# Interface
st.title(" Fake News Detector")
st.write("Collez un article de news ci-dessous pour vérifier s'il est vrai ou faux")

# Zone de texte
article = st.text_area("Article:", height=200)

if st.button("Analyser"):
    if article:
        # Chargement du modèle
        model = load_model()

        # Prédiction
        prediction = model.predict([article])
        probability = model.predict_proba([article])

        # Affichage des résultats
        col1, col2 = st.columns(2)

        with col1:
            if prediction[0] == 0:
                st.success("✅ Cet article semble **VRAI**")
            else:
                st.error("❌ Cet article semble **FAUX**")

        with col2:
            st.metric("Confiance", f"{max(probability[0])*100:.1f}%")

        # Détails
        with st.expander("Voir les détails"):
            st.write(f"Probabilité d'être vrai: {probability[0][0]*100:.2f}%")
            st.write(f"Probabilité d'être faux: {probability[0][1]*100:.2f}%")

            # Features importantes
            st.subheader("Mots clés influents")
            # Logique pour extraire les features importantes...

    else:
        st.warning("Veuillez entrer un article à analyser")

# Section informations
with st.sidebar:
    st.header("ℹ️ Informations")
    st.write("""
    Ce modèle a été entraîné sur un dataset de:
    - 23,481 articles vrais
    - 23,481 articles faux

    **Précision**: 95.2%
    **Recall**: 95.1%
    **F1-Score**: 95.1%
    """
    )
'''

# -%%
# Sauvegarde du code Streamlit
with open('app.py', 'w') as f:
    f.write(streamlit_code)

print("Code Streamlit généré dans 'app.py'")
print("\nPour exécuter l'application:")
print("1. Installe Streamlit: pip install streamlit")
print("2. Exécute: streamlit run app.py")

# %% [markdown]
# ### 14. Conclusion et Analyse des Résultats
# -%%
# Synthèse finale
print("=" * 60)
print("SYNTHÈSE DU PROJET - DÉTECTION DE FAKE NEWS")
print("=" * 60)

print(f"\n **Performances Finales:**")
print(f"   - Modèle: Random Forest Optimisé")
print(f"   - Accuracy: {accuracy_score(y_test, y_pred_best):.3f}")
print(f"   - F1-Score: {f1_score(y_test, y_pred_best):.3f}")
print(f"   - ROC-AUC: {roc_auc_score(y_test, best_rf.predict_proba(X_test_combined)[:, 1]):.3f}")

print(f"\n **Insights Clés:**")
print(f"   1. Les articles faux ont en moyenne:")
print(f"      - Plus de points d'exclamation")
print(f"      - Un ratio de majuscules plus élevé")
print(f"      - Un vocabulaire moins diversifié")

print(f"\n   2. Features les plus importantes:")
print(f"      - Présence de mots émotionnels")
print(f"      - Longueur des phrases")
print(f"      - Certains bigrammes spécifiques")

print(f"\n **Recommandations:**")
print(f"   1. Pour améliorer le modèle:")
print(f"      - Ajouter plus de données d'entraînement")
print(f"      - Essayer d'autres architectures (BERT, RoBERTa)")
print(f"      - Ajouter des features sémantiques")




